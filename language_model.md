# Comments

## Employ LM in the downstream task

&#x1F4D8; **Context-specific language modeling for human trafficking detection from online advertisements**

+ Conference: ACL2019
+ Summarization: This paper introduces a new task called human trafficking detection, which is a text classification problem. In this work, they employ the BERT as a kind of feature model to encode the text.
+ Weakness: 


&#x1F4D8; **Fine-tuning Pre-Trained Transformer Language Models to Distantly Supervised Relation Extraction**

+ Conference: ACL2019
+ Summarization: Relation Extraction is a task of identifying the relations between concepts appeared in text. The distantly supervised RE is a multi-instances learning problem, which uses a set of sentences that contains the same concepts to predict the relation between them.
+ Weakness:

&#x1F4D8; **Stochastic Tokenization with a Language Model for Neural Text Classification**

+ Conference: ACL2019
+ Summarization: This paper uses a LM to find a more appropriate segmentaion of sentence for text classification
+ Weakness:

## Improve the performence

&#x1F4D8; **Breaking the Softmax Bottleneck: A High-Rank RNN Language Model**

+ Conference: ICLR2018
+ Summarization: ameliorate the bottleneck of softmax
+ Weakness:

&#x1F4D8; **Regularizing and Optimizing LSTM Language Models**

+ Conference: ICLR2018
+ Summarization: regularization
+ Weakness:

&#x1F4D8; **Improved Language Modeling by Decoding the Past**

+ Conference: ACL2019
+ Summarization: regularization while decoding
+ Weakness:

&#x1F4D8; **Training Hybrid Language Models by Marginalizing over Segmentations**

+ Conference: ACL2019
+ Summarization: A word may have several legitimate segmentations, such as "ABC" could be segmented to "AB C" and "A BC". And this paper take the set of valid segmentations into account.
+ Weakness:

&#x1F4D8; **Improving Neural Language Models by Segmenting, Attending, and Predicting the Future**

+ Conference: ACL2019
+ Summarization: Using the information of context-to-phrase to help the word prediction.
+ Weakness:
+ Note: This paper gives a brief of previous works of neural language models (NLMs), which claims that the NLMs are trained to factorize the context-to-word relation matrices.


&#x1F4D8; **Better Character Language Modeling through Morphology**

+ Conference: ACL2019
+ Summarization: Character LM (CLM) with morphology supervision task
+ Weakness:

&#x1F4D8; **Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context**

+ Conference: ACL2019
+ Summarization: The Transformer-XL is designed to improve the capability of modeling long-term dependency of Transfomer.
+ Weakness:
+ Note: A short brief of LM, especially the line of self-attention models.


