&#x1F4D8; **Neural Language Modeling by Jointly Learning Syntax and Lexicon**

+ Conference: ICLR2018
+ Summarization:
+ Weakness:

&#x1F4D8; **Grammar Induction with Neural Language Models: An Unusual Replication**

+ Conference: EMNLP2018
+ Summarization:
+ Weakness:

&#x1F4D8; **Ordered Neurons: Integrating Tree Structures into Recurrent Neural Networks**

+ Conference: ICLR2019
+ Summarization:
+ Weakness:


&#x1F4D8; **Unsupervised Recurrent Neural Network Grammars**

+ Conference: NAACL2019
+ Summarization:
+ Weakness:

&#x1F4D8; **Scalable Syntax-Aware Language Models Using Knowledge Distillation**

+ Conference: ACL2019
+ Summarization: They try to transfer the knowledge learned by the syntactic language model to an LSTM language model using an Knowledge Distillation technique
+ Weakness: 


&#x1F4D8; **Inducing Constituency Trees through Neural Machine Translation**

+ Conference: arxiv
+ Summarization:
+ Weakness:


&#x1F4D8; **Tree Transformer: Integrating Tree Structures into Self-Attention**

+ Conference: EMNLP2019
+ Summarization:
+ Weakness:

&#x1F4D8; **PaLM: A Hybrid Parser and Language Model**

+ Conference: EMNLP2019
+ Summarization:
+ Weakness:
